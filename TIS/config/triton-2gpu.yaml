apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-3-8b
  namespace: llama
  annotations:
    "sidecar.istio.io/inject": "true"
spec:
  predictor:
    model:
      modelFormat:
        name: triton
        version: "2"
      runtime: triton-trtllm
#      storageUri: pvc://llm-data-pvc/
      volumeMounts:
        - name: llm-data-pvc
          mountPath: /mnt/models/models   # 如果用/mnt/models会重名，KServe的BUG，暂时我这边不解决
          subPath: models
          readOnly: false
        - name: llm-data-pvc
          mountPath: /mnt/models/checkpoints   # output converted checkpoints
          subPath: checkpoints
          readOnly: false
        - name: llm-data-pvc
          mountPath: /mnt/models/engines       # output built engines
          subPath: engines
          readOnly: false
        - name: secret-volume
          mountPath: /etc/secret
      name: kserve-container
      resources:
        limits:
          nvidia.com/v100: "2"
      command:
        - "/bin/bash"
        - "-c"
        - "git -C /code/REServe/Initializer pull && /code/REServe/Initializer/main.sh run --tp 2"
    volumes:
      - name: llm-data-pvc
        persistentVolumeClaim:
          claimName: llm-data-pvc
          readOnly: false
      - name: secret-volume
        secret:
          secretName: rook-ceph-admin-keyring